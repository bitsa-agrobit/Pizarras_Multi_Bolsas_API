name: Daily scrape to GitHub Pages

on:
  workflow_dispatch:
  schedule:
    - cron: '15 12 * * *'

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build:
    runs-on: [self-hosted]
    env:
      PATH: /opt/homebrew/bin:/usr/local/bin:${{ env.PATH }}
      PY: /opt/homebrew/bin/python3
      PIP: /opt/homebrew/bin/pip3
      PYTHONPATH: app
      SCRAPER_DRIVER: playwright
    steps:
      - uses: actions/checkout@v4

      - name: Check Python
        run: |
          "$PY" --version
          "$PIP" --version || true

      - name: Install deps (CI-light, sin oracledb)
        run: |
          "$PY" -m pip install --upgrade pip
          # instala sÃ³lo lo que el scraper usa:
          "$PIP" install fastapi==0.115.6 requests==2.32.3 beautifulsoup4==4.12.3 lxml==5.2.2 html5lib==1.1 pandas==2.2.2 playwright==1.55.0

          # En macOS: SIN --with-deps
          "$PY" -m playwright install chromium

      - name: Make index.html
        run: |
          mkdir -p public
          cat > public/index.html <<'HTML'
          <!doctype html><meta charset="utf-8">
          <title>Pizarras cache</title>
          <h1>Pizarras cache (GitHub Pages)</h1>
          <ul>
            <li><a href="cotizaciones_rosario.json">cotizaciones_rosario.json</a></li>
            <li><a href="cotizaciones_bahia.json">cotizaciones_bahia.json</a></li>
            <li><a href="cotizaciones_cordoba.json">cotizaciones_cordoba.json</a></li>
            <li><a href="cotizaciones_quequen.json">cotizaciones_quequen.json</a></li>
            <li><a href="cotizaciones_darsena.json">cotizaciones_darsena.json</a></li>
            <li><a href="cotizaciones_locales.json">cotizaciones_locales.json</a></li>
          </ul>
          HTML

      - name: Run scraper
        shell: bash -l {0}
        run: |
          "$PY" - <<'PY'
          import os, json
          from pathlib import Path
          os.environ["SCRAPER_DRIVER"] = "playwright"
          from app.main import cotizaciones

          def dump(plaza, only_base):
              data = cotizaciones(plaza, only_base)
              Path("public").mkdir(exist_ok=True)
              suf = "_base" if int(only_base)==1 else ""
              fn = f"public/cotizaciones_{data['plaza']}{suf}.json"
              with open(fn, "w", encoding="utf-8") as f:
                  json.dump(data, f, ensure_ascii=False)
              print(f"Wrote {fn} (items={len(data.get('items',[]))}, error={data.get('error')})")

          for plaza in ["rosario","bahia","cordoba","quequen","darsena","locales"]:
              dump(plaza, 1)
              dump(plaza, 0)
          print("OK")
          PY

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

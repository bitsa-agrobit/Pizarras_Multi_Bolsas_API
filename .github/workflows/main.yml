name: Daily scrape to GitHub Pages

on:
  workflow_dispatch:
  schedule:
    - cron: '15 12 * * *'   # diario 12:15 UTC
  push:
    paths:
      - 'backend/**'
      - '.github/workflows/main.yml'

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install --with-deps chromium

      - name: Run scraper
        env:
          SCRAPER_DRIVER: playwright
          PYTHONPATH: backend
        run: |
          python - <<'PY'
import json
from pathlib import Path
from main import cotizaciones
def dump(plaza, only_base):
    data = cotizaciones(plaza, only_base)
    Path("public").mkdir(exist_ok=True)
    fn = f"public/cotizaciones_{data['plaza']}.json"
    with open(fn,"w",encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False)
for plaza in ["rosario","bahia","cordoba","quequen","darsena","locales"]:
    dump(plaza, 1)
    dump(plaza, 0)
print("OK")
PY

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

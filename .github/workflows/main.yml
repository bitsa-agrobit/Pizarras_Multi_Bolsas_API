name: Daily scrape to GitHub Pages

on:
  workflow_dispatch:
  schedule:
    - cron: '15 12 * * *'  # 12:15 UTC

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true   # evita carreras en paralelo

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      PYTHONPATH: app
      SCRAPER_DRIVER: playwright
      HOMEBREW_NO_AUTO_UPDATE: 1
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show runner info
        shell: bash
        run: |
          set -euxo pipefail
          uname -a || true
          command -v python3
          python3 --version
          python3 -m pip --version || true
          ls -la

      - name: Install deps (sin oracledb)
        shell: bash
        run: |
          set -euxo pipefail
          python3 -m venv .venv
          source .venv/bin/activate
          python3 -m pip install --upgrade pip
          python3 -m pip install \
            fastapi==0.115.6 \
            requests==2.32.3 \
            beautifulsoup4==4.12.3 \
            lxml==5.2.2 \
            html5lib==1.1 \
            pandas==2.2.2 \
            playwright==1.55.0
          python3 -m playwright install chromium

      - name: Make index.html
        shell: bash
        run: |
          set -euxo pipefail
          mkdir -p public
          cat > public/index.html <<'HTML'
          <!doctype html><meta charset="utf-8">
          <title>Pizarras cache</title>
          <h1>Pizarras cache (GitHub Pages)</h1>
          <ul>
            <li><a href="cotizaciones_rosario.json">cotizaciones_rosario.json</a></li>
            <li><a href="cotizaciones_bahia.json">cotizaciones_bahia.json</a></li>
            <li><a href="cotizaciones_cordoba.json">cotizaciones_cordoba.json</a></li>
            <li><a href="cotizaciones_quequen.json">cotizaciones_quequen.json</a></li>
            <li><a href="cotizaciones_darsena.json">cotizaciones_darsena.json</a></li>
            <li><a href="cotizaciones_locales.json">cotizaciones_locales.json</a></li>
          </ul>
          HTML

      # Jitter inicial para no pegarle siempre a la misma hora exacta
      - name: Random jitter
        shell: bash
        run: |
          JITTER=$(( (RANDOM % 60) + 20 ))  # 20–79s
          echo "Sleeping ${JITTER}s before scraping..."
          sleep "${JITTER}"

      - name: Run scraper (Playwright) with retries & preserve-on-fail
        shell: bash
        env:
          SCRAPER_DRIVER: playwright
        run: |
          set -euxo pipefail
          source .venv/bin/activate
          python3 - <<'PY'
          import os, sys, json, time
          from pathlib import Path
          os.environ["SCRAPER_DRIVER"] = "playwright"  # no tocamos tu app
          sys.path.insert(0, "app")
          from main import cotizaciones

          outdir = Path("public")
          outdir.mkdir(exist_ok=True)

          def write_if_ok(fn: Path, data: dict):
            items = data.get("items") or []
            if items:
              fn.write_text(json.dumps(data, ensure_ascii=False), encoding="utf-8")
              print(f"Wrote {fn} (items={len(items)})")
              return True
            else:
              if fn.exists():
                print(f"[WARN] vacío => preservo {fn} existente")
              else:
                print(f"[WARN] vacío y no existe {fn}; no creo archivo")
              return False

          def scrape_with_retries(plaza: str, only_base: int, attempts=3, sleep_s=2.0):
            suf = "_base" if int(only_base)==1 else ""
            # Usamos el nombre tal como lo generabas antes
            fn = outdir / f"cotizaciones_{plaza}{suf}.json"
            last = None
            for i in range(1, attempts+1):
              try:
                data = cotizaciones(plaza, only_base)
                last = data
                if write_if_ok(fn, data):
                  return True
                else:
                  print(f"[TRY {i}/{attempts}] {plaza} only_base={only_base} vacío, reintento en {sleep_s}s…")
                  time.sleep(sleep_s)
              except Exception as e:
                print(f"[TRY {i}/{attempts}] error {plaza} only_base={only_base}: {e} ; retry in {sleep_s}s")
                time.sleep(sleep_s)
            # último intento no exitoso: si no hay archivo previo, al menos dejamos el último JSON (aunque vacío) para diagnosticar
            if not fn.exists() and last is not None:
              fn.write_text(json.dumps(last, ensure_ascii=False), encoding="utf-8")
              print(f"[INFO] guardé último resultado (vacío) sólo para diagnóstico: {fn}")
            return False

          plazas = ["rosario","bahia","cordoba","quequen","darsena","locales"]
          for p in plazas:
            scrape_with_retries(p, 1, attempts=3, sleep_s=3.0)  # base
            time.sleep(1.5)
            scrape_with_retries(p, 0, attempts=3, sleep_s=3.0)  # full
            time.sleep(2.0)

          print("OK")
          PY

      - name: Verify artifact contents
        shell: bash
        run: |
          set -euxo pipefail
          [ -d public ] || (echo "public/ no existe"; exit 1)
          find public -type f -maxdepth 2 -print
          # Debe existir al menos un archivo
          test -n "$(find public -type f -maxdepth 2 | head -n1)"

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ${{ github.workspace }}/public

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
